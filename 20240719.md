# 2024년 7월 19일

### AI 공부
+ **마르코프 의사결정 모델(MDP)**
    + 인공지능이 **학습하고자 하는 방식을 공식화하여 추론하는  모델**
    + **벽돌게임**을 예로 들어 설명하자면
        |MDP|예시|
        |---|---|
        |**Environment(환경)**|벽돌깨기|
        |**State(상태)**|공의 방향,위치,벽돌의 존재유무 등|
        |**Action(행동)**|바를 왼쪽,오른쪽으로 이동시키는 등의 행동|
        |**Reward(보상)**|**Action**의 결과로 나오는 점수||
        |**Policy(정책)**|**Action**이 **Environment**를 변화시키고 **State**값이 변하게 되며 이를 통해 Agent가 다시 **Action**을 선택하는 것|
    + 위 표의 요소들에 따라 MDP의 **Process**가 결정된다
    + 이렇게 결정된 여러 **Process** 중 하나의 **Episode**가 상태,행동 그리고 그로 인한 **보상**이라는 **Sequence**를 형성한다
+ **Discounted Future Reward**
    + 오랫동안 좋은 수행능력을 **유지**하기 위해 당장의 Reward를 포함해 미래에도 좋은 Reward를 받을 수 있도록 해야한다
    + MDP의 Process가 한 번 실행이 되는 것을 보면 하나의 **Episode**의 **Reward**를 예측가능하다
    + 특정 시점에서 얻는 **Reward**양을 다음의 수식으로 계산 가능하다
    $$
    R_t = \sum_{t' = t}^{T} r^{t'-t} r_{t'}
    $$
    + 하지만 Environment는 **확률적**이기 때문에 다음 상황에서 좋은 Reward를 섣불리 예상할 수 없다
    + 오히려 미래로 갈수록 지금 당장의 Reward의 가치는 점전 **감소**하고 **케이스가 나뉘게 된다**
    + 이를 위해 **Reward Factor**를 사용하여 미래의 Reward를 **차감**시켜 나간다
    + 일반적으론 Reward Factor은 **0~1**의 범위를 가지게 되는데 이 **차감변수**를 **0**에 가깝게 둔다면 **근시안적인 목표**를 가지고 지금의 Reward를 극대화 하는 방향으로 초점을 두고 Environment가 변하지 않으며 같은 Action으로 같은 Reward가 제공된다면 **1**로 설정하면 된다
    + 현재와 미래 Reward 사이에서 균형을 이루고 싶다면 보통은 **0.9** 정도의 값을 정하면 된다
+ **벨만 방정식(Bellman Equation)**
    + **MDP 에서는** 벨만 방정식을 **가치함수(기대함수)** 라 정의할 수 있다
    + Agent가 어떠한 상태로 진행할 경우의 받을 **보상의 합**에 대한 **기대값**이라고 할 수 있다
    + 가치함수는 현재 Agent의 상태에 영향을 받는데 이 Policy를 반영한 것이 **벨만 방정식**이다